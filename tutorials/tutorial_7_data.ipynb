{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial 7 — Working with `qcom.data`\n",
        "\n",
        "This notebook shows how to use the utilities in **`qcom.data`**:\n",
        "\n",
        "- `qcom.data.ops` — normalize counts → probabilities, truncate small entries, and print top states.\n",
        "- `qcom.data.sampling` — resample (generate synthetic datasets) and combine datasets safely.\n",
        "- `qcom.data.noise` — add classical readout noise (Monte Carlo) and optionally **mitigate** with `mthree` using simple per-qubit error rates.\n",
        "\n",
        "The examples use small toy datasets, so you can run everything quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from qcom.data.ops import normalize_to_probabilities, truncate_probabilities, print_most_probable_data\n",
        "from qcom.data.sampling import sample_data, combine_datasets\n",
        "from qcom.data.noise import introduce_error, m3_mitigate_counts_from_rates\n",
        "\n",
        "import random\n",
        "from pprint import pprint\n",
        "\n",
        "# For reproducibility in sampling/noise demos\n",
        "random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Start with a small counts dictionary\n",
        "Assume you obtained integer **counts** from an experiment or simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total shots: 1000\n",
            "{'00': 510, '01': 230, '10': 220, '11': 40}\n"
          ]
        }
      ],
      "source": [
        "counts = {\n",
        "    \"00\": 510,\n",
        "    \"01\": 230,\n",
        "    \"10\": 220,\n",
        "    \"11\": 40,\n",
        "}\n",
        "total_count = sum(counts.values())\n",
        "print(\"Total shots:\", total_count)\n",
        "pprint(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Normalize to probabilities & inspect the most likely states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sum of probabilities: 1.0\n",
            "Top 4 Most probable bit strings:\n",
            "1.  Bit string: 00, Probability: 0.51000000\n",
            "2.  Bit string: 01, Probability: 0.23000000\n",
            "3.  Bit string: 10, Probability: 0.22000000\n",
            "4.  Bit string: 11, Probability: 0.04000000\n"
          ]
        }
      ],
      "source": [
        "probs = normalize_to_probabilities(counts, total_count)\n",
        "print(\"Sum of probabilities:\", sum(probs.values()))\n",
        "print_most_probable_data(probs, n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Truncate small probabilities (no renormalization)\n",
        "You might want to ignore a tail of tiny entries during reporting or plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kept entries ≥ 0.05:\n",
            "{'00': 0.51, '01': 0.23, '10': 0.22}\n",
            "Sum after truncation (no renorm): 0.96\n"
          ]
        }
      ],
      "source": [
        "truncated = truncate_probabilities(probs, threshold=0.05)\n",
        "print(\"Kept entries ≥ 0.05:\")\n",
        "pprint(truncated)\n",
        "print(\"Sum after truncation (no renorm):\", sum(truncated.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Sampling: generate a synthetic dataset from counts\n",
        "Use `sample_data` to resample a new integer-count dataset of a desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sampled: 2000\n",
            "{'00': 994, '01': 465, '10': 464, '11': 77}\n"
          ]
        }
      ],
      "source": [
        "sample_size = 2000\n",
        "sampled_counts = sample_data(counts, total_count=total_count, sample_size=sample_size, show_progress=False)\n",
        "print(\"Total sampled:\", sum(sampled_counts.values()))\n",
        "pprint(sampled_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Combine datasets safely\n",
        "You can merge **two probability dictionaries** (they will be renormalized) **or** merge **two counts dictionaries** directly. Mixing a prob-dict with a counts-dict raises an error to prevent silent mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged counts total: 1100\n",
            "{'00': 560, '01': 270, '10': 230, '11': 40}\n",
            "Merged probs sum: 1.0\n",
            "Top 4 Most probable bit strings:\n",
            "1.  Bit string: 00, Probability: 0.50500000\n",
            "2.  Bit string: 01, Probability: 0.31500000\n",
            "3.  Bit string: 10, Probability: 0.16000000\n",
            "4.  Bit string: 11, Probability: 0.02000000\n"
          ]
        }
      ],
      "source": [
        "# Merge two counts dicts\n",
        "more_counts = {\"00\": 50, \"01\": 40, \"10\": 10, \"11\": 0}\n",
        "merged_counts = combine_datasets(counts, more_counts, show_progress=False)\n",
        "print(\"Merged counts total:\", sum(merged_counts.values()))\n",
        "pprint(merged_counts)\n",
        "\n",
        "# Merge two probability dicts\n",
        "probs_a = normalize_to_probabilities(counts, total_count)\n",
        "probs_b = normalize_to_probabilities(more_counts, sum(more_counts.values()))\n",
        "merged_probs = combine_datasets(probs_a, probs_b, show_progress=False)\n",
        "print(\"Merged probs sum:\", sum(merged_probs.values()))\n",
        "print_most_probable_data(merged_probs, n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Add classical readout noise (Monte Carlo)\n",
        "`introduce_error` simulates independent per-qubit flips **on the measured bitstrings**.\n",
        "\n",
        "- `ground_rate`: probability a measured `0` flips to `1`.\n",
        "- `excited_rate`: probability a measured `1` flips to `0`.\n",
        "\n",
        "You can pass a **single float** (same for all qubits), a **list/tuple of per-qubit rates**, or a **{index → rate} mapping** for sparse overrides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noisy counts (global rates) total: 1000\n",
            "{'00': 542, '01': 212, '10': 207, '11': 39}\n",
            "Noisy counts (per-qubit rates) total: 1000\n",
            "{'00': 544, '01': 201, '10': 216, '11': 39}\n"
          ]
        }
      ],
      "source": [
        "# Global scalar rates\n",
        "noisy_counts_global = introduce_error(counts, ground_rate=0.01, excited_rate=0.08, seed=1234)\n",
        "print(\"Noisy counts (global rates) total:\", sum(noisy_counts_global.values()))\n",
        "pprint(noisy_counts_global)\n",
        "\n",
        "# Per-qubit rates (here 2 qubits)\n",
        "noisy_counts_per_qubit = introduce_error(counts, ground_rate=[0.02, 0.00], excited_rate=[0.05, 0.10], seed=1234)\n",
        "print(\"Noisy counts (per-qubit rates) total:\", sum(noisy_counts_per_qubit.values()))\n",
        "pprint(noisy_counts_per_qubit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Optional: Readout **mitigation** with `mthree`\n",
        "If you have the `mthree` package installed (`pip install qiskit-addon-mthree`), you can mitigate readout error using only two per-qubit numbers: `ground_rate` (0→1) and `excited_rate` (1→0). Under the hood, we build the confusion matrices and apply `M3Mitigation`.\n",
        "\n",
        "> The function returns **quasi-probabilities** (may include tiny negative values internally). We clip and renormalize for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mitigated distribution (sum= 0.9999999999999999 )\n",
            "Top 4 Most probable bit strings:\n",
            "1.  Bit string: 00, Probability: 0.51703898\n",
            "2.  Bit string: 01, Probability: 0.22362034\n",
            "3.  Bit string: 10, Probability: 0.21812584\n",
            "4.  Bit string: 11, Probability: 0.04121483\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import mthree  # noqa: F401\n",
        "    have_m3 = True\n",
        "except Exception:\n",
        "    have_m3 = False\n",
        "\n",
        "if have_m3:\n",
        "    mitigated = m3_mitigate_counts_from_rates(\n",
        "        noisy_counts_global,\n",
        "        ground_rate=0.01,\n",
        "        excited_rate=0.08,\n",
        "        qubits=None,  # defaults to [0..N-1]\n",
        "    )\n",
        "    print(\"Mitigated distribution (sum=\", sum(mitigated.values()), \")\")\n",
        "    print_most_probable_data(mitigated, n=4)\n",
        "else:\n",
        "    print(\"mthree not installed; skipping mitigation demo. Install with: pip install qiskit-addon-mthree\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Where to go next\n",
        "- Use `qcom.io` (JSON/text/Parquet) to load/save datasets, then process them with the functions shown here.\n",
        "- Try your own per-qubit error maps to stress-test mitigation.\n",
        "- For larger bitstrings, prefer working with truncated or sparse dicts when printing/plotting."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
