{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Document\n",
    "This document details the use of the QCOM package. The goal is a python package that allows users to interact with Aquila and DMRG data seemlessly and prevent repetitive function writing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from DMRG files\n",
    "If you have data in text files with the form\n",
    "\n",
    "state : count\n",
    "\n",
    "state : count\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Then the function below allows you to easily obtain and return the data as a dictionary and additionally the total count of the values (equals 1 if the values are probabilities) using the file path as input.\n",
    "\n",
    "This function also has a built in progress manager so you can estimate how long the data takes to load in. This is helpful for larger datasets. To track progress use the \"show_progress\" flag and set it equal to True. This flag is always the last parameter passed into the function call. It is set to False by default. \n",
    "\n",
    "Note: \n",
    "\n",
    "You must set the flag in the function call itself like shown below if the function has multiple arguments before show_progress. You can try for yourself but if you just do show_progress = True on a separate line and pass that into the function it will not show progress. This is because parse_file also takes in an additional two arguments, one of which (sample_size) can be represented as a boolean. Thus if you just pass file_path and show_progress with show_progress defined on a previous line, it will read show_progress as the sample size input and it will not show progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip install\n",
    "\n",
    "Make sure you pip install qcom before trying the tutorial. If you haven't already running the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "pip install qcom\n",
    "```\n",
    "\n",
    "Confirm that qcom has been succesfully installed and restart your jupyter before trying to import qcom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the qcom module\n",
    "\n",
    "import qcom as qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Parsing file...\n",
      "Task: Parsing file | Progress: 100.00% | Elapsed: 0.00s | Remaining: 0.00s      \n",
      "Completed: Parsing file. Elapsed time: 0.00 seconds.\n",
      "Total count:  1000000000.0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../example_data/1_billion_3.0_4_rungs.txt\"\n",
    "\n",
    "# Process without sampling\n",
    "processed_data, total_count = qc.parse_file(file_path, show_progress = True)\n",
    "\n",
    "print(\"Total count: \", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files\n",
    "\n",
    "The previous code demonstrated how to parse text files, the following code will tell you how to parse parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Parsing Parquet file...\n",
      "Task: Parsing Parquet file | Progress: 100.00% | Elapsed: 0.02s | Remaining: 0.00s\n",
      "Completed: Parsing Parquet file. Elapsed time: 0.03 seconds.\n",
      "Total count:  10000000.0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../example_data/1_billion_counts_3.0_4_rungs.parquet\"\n",
    "\n",
    "# Process without sampling\n",
    "\n",
    "processed_data = qc.parse_parq(file_path, show_progress = True)\n",
    "\n",
    "total_count = sum(list(processed_data.values()))\n",
    "print(\"Total count: \", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most probable states\n",
    "This is great but it's hard to confirm for very large datasets. To confirm the result we might just be interested in the 10 most probable states. The function below allows us to print out the n most probable states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 01000010, Probability: 389245564.00000000\n",
      " 2.  Bit string: 10000001, Probability: 389223579.00000000\n",
      " 3.  Bit string: 01000001, Probability: 48778624.00000000\n",
      " 4.  Bit string: 10000010, Probability: 48777628.00000000\n",
      " 5.  Bit string: 01000000, Probability: 20562997.00000000\n",
      " 6.  Bit string: 00000001, Probability: 20561460.00000000\n",
      " 7.  Bit string: 00000010, Probability: 20550353.00000000\n",
      " 8.  Bit string: 10000000, Probability: 20549831.00000000\n",
      " 9.  Bit string: 10000100, Probability: 4983517.00000000\n",
      "10.  Bit string: 00100001, Probability: 4981922.00000000\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "qc.print_most_probable_data(processed_data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "If I want to take a random sample of the larger data into a smaller amount I can do so using the sample_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Sampling data...\n",
      "Task: Sampling data | Progress: 100.00% | Elapsed: 0.01s | Remaining: 0.00s     \n",
      "Completed: Sampling data. Elapsed time: 0.01 seconds.\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 01000010, Probability: 0.40700000\n",
      " 2.  Bit string: 10000001, Probability: 0.36700000\n",
      " 3.  Bit string: 10000010, Probability: 0.04800000\n",
      " 4.  Bit string: 01000001, Probability: 0.04600000\n",
      " 5.  Bit string: 10000000, Probability: 0.02800000\n",
      " 6.  Bit string: 01000000, Probability: 0.02200000\n",
      " 7.  Bit string: 00000001, Probability: 0.01900000\n",
      " 8.  Bit string: 00000010, Probability: 0.01700000\n",
      " 9.  Bit string: 00100001, Probability: 0.00700000\n",
      "10.  Bit string: 10000011, Probability: 0.00600000\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "sampled_data = qc.sample_data(processed_data, total_count, sample_size, show_progress= True)\n",
    "qc.print_most_probable_data(sampled_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample as you parse\n",
    "It's actually more efficient to sample as we parse the set. So if you know before hand you only care about 100 randomly sample states it makes no sense to parse the whole thing and then sample it down to 100. Instead as you are parsing you will only grab 100 states. We can do this using the parse_file function by adding the sample_size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"1_billion_3.0_4_rungs.txt\"\n",
    "\n",
    "# Process with sampling\n",
    "\n",
    "sample_size = 1000\n",
    "processed_data, total_count = qc.parse_file(file_path, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "The Aquila device has a readout error rate of 0.08 for the excited state and 0.01 for the ground state. We can simulate this error on our data using the following function\n",
    "\n",
    "NOTE: The current function assumes the default values of 0.08 and 0.01. These are taken as parameters so if future error rates change then we can accurately model those as well. Thus, you do not technically need to pass ground_rate and excited_rate into the function. Although it is good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing errors to the data...\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 10000000, Probability: 0.09090909\n",
      " 2.  Bit string: 01000010, Probability: 0.04545455\n",
      " 3.  Bit string: 00000010, Probability: 0.04545455\n",
      " 4.  Bit string: 10000010, Probability: 0.04545455\n",
      " 5.  Bit string: 00010010, Probability: 0.04545455\n",
      " 6.  Bit string: 01001001, Probability: 0.04545455\n",
      " 7.  Bit string: 01000001, Probability: 0.04545455\n",
      " 8.  Bit string: 10000100, Probability: 0.04545455\n",
      " 9.  Bit string: 01000000, Probability: 0.04545455\n",
      "10.  Bit string: 10000011, Probability: 0.04545455\n"
     ]
    }
   ],
   "source": [
    "ground_rate = 0.01\n",
    "excited_rate = 0.08\n",
    "\n",
    "error_data = qc.introduce_error_data(sampled_data, total_count, ground_rate, excited_rate)\n",
    "qc.print_most_probable_data(error_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data\n",
    "Say you have two data sets and you want to combine them. We can do this using the following funciton, however there are some rules. You can either combine two datasets of probabilities or two datasets of counts. You cannot combine a dataset of probabilities and a dataset of counts as this would make normalizing impossible. Additionally, if you combine two probabilities, the function will automatically normalize. If you combine two datasets of counts, the function will NOT normalize. This is so if users want to combine say 100 sets of count data, they can do so without a problem. They simply need to normalize afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10000010': 0.04772727272727273, '10001000': 0.02322727272727273, '00000100': 0.02372727272727273, '01000010': 0.23322727272727273, '10000110': 0.02372727272727273, '10000001': 0.1935, '11000010': 0.02322727272727273, '01000011': 0.02372727272727273, '11000001': 0.025727272727272727, '01000000': 0.03172727272727273, '00010010': 0.02422727272727273, '00000001': 0.007, '10001001': 0.02372727272727273, '01001000': 0.026227272727272728, '00000010': 0.03172727272727273, '00100011': 0.022727272727272728, '01001001': 0.022727272727272728, '10000000': 0.050954545454545454, '01000001': 0.043727272727272726, '10000100': 0.02422727272727273, '00000000': 0.02372727272727273, '10000011': 0.025227272727272727, '00100001': 0.001, '00000011': 0.02322727272727273}\n"
     ]
    }
   ],
   "source": [
    "combined_data = qc.combine_datasets(sampled_data, error_data)\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to txt File\n",
    "If we have a set of data we would like to save to a file we can do so using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"error_data.txt\"\n",
    "qc.save_data(processed_data, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Info Measure\n",
    "\n",
    "In the following we will be using the probabilities to get classical information measured about the system such as Shannon entropies, Reduced Shannon Entropies, Mutual Information, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "Let's say you have a ladder configuration of atoms with 4 rungs and you want to compute the shannon entropy of the entire dataset. You can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Parsing file...\n",
      "Task: Parsing file | Progress: 100.00% | Elapsed: 0.00s | Remaining: 0.00s      \n",
      "Completed: Parsing file. Elapsed time: 0.00 seconds.\n",
      "Shannon entropy:  1.6021996352622572\n"
     ]
    }
   ],
   "source": [
    "# choose a dataset\n",
    "file_path = \"../example_data/1_billion_3.0_4_rungs.txt\"\n",
    "\n",
    "# Load the data\n",
    "processed_data, total_count = qc.parse_file(file_path, show_progress = True)\n",
    "\n",
    "# compute the shannon entropy\n",
    "shannon_entropy = qc.compute_shannon_entropy(processed_data)\n",
    "print(\"Shannon entropy: \", shannon_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhaps you want the reduced shannon entropy of just a portion of the atoms\n",
    "\n",
    "You can define your configuration like below. The way you do your indexing depends on how you hamiltonian is constructed or how your experimental data is recorded. The convention for the 4 rung example data has indices like so \n",
    "\n",
    "1 3 5 7 <br>\n",
    "0 2 4 6\n",
    "\n",
    "So if I choose to have the \"in\" portion (portion A) have the atoms 0,1,2 and consequentily the \"out\" portion (portion B) be 3,4,5,6,7. The array will look like this:\n",
    "\n",
    "A B B B <br>\n",
    "A A B B \n",
    "\n",
    "Where A means in and B means out. In the code instead of A and B we use 1 and 0. I used letters in this example so as not to confuse you about the indexing convention.\n",
    "\n",
    "In the code you designate your configuration with 0's and 1's and then choose a target region (0 or 1) to choose which region to obtain the reduced shannon entropy for. In the code below I want the region with the 1's so I set the target region to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Shannon entropy:  0.9284886172395846\n"
     ]
    }
   ],
   "source": [
    "# compute reduced shannon entropy on region A of processed data\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "reduced_shannon_entropy = qc.compute_reduced_shannon_entropy(processed_data, configuration, 1)\n",
    "print(\"Reduced Shannon entropy: \", reduced_shannon_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously I can do the opposite measure by simply flipping the target region to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Shannon entropy:  1.0002980229566578\n"
     ]
    }
   ],
   "source": [
    "# compute reduced shannon entropy on region B of processed data\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "reduced_shannon_entropy = qc.compute_reduced_shannon_entropy(processed_data, configuration, 0)\n",
    "print(\"Reduced Shannon entropy: \", reduced_shannon_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "The mutual information between two seperate regions A and B is defined as:\n",
    "\n",
    "MI =  $S_{x}(A) + S_{x}(B) - S_{x}(AB)$\n",
    "\n",
    "Where $S_x(c)$ is the shannon entropy over region c. \n",
    "\n",
    "You can compute this measure immedietly or you can do so in parts like previously. I will show they yield the same result.\n",
    "\n",
    "Also note there is no target region for this code as you are compute the reduced shannon entropy of both regions regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information:  0.3265870049339852\n",
      "Shannon entropy:  1.6021996352622572\n",
      "Reduced Shannon entropy:  1.0002980229566578\n",
      "Reduced Shannon entropy:  1.0002980229566578\n",
      "Mutual information computed with steps:  0.3265870049339852\n"
     ]
    }
   ],
   "source": [
    "# compute MI between region A and B of processed data\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "mutual_information = qc.compute_mutual_information(processed_data, configuration)\n",
    "print(\"Mutual information: \", mutual_information)\n",
    "\n",
    "# compute the shannon entropy\n",
    "shannon_entropy = qc.compute_shannon_entropy(processed_data)\n",
    "print(\"Shannon entropy: \", shannon_entropy)\n",
    "\n",
    "# compute reduced shannon entropy on region A of processed data\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "reduced_shannon_entropy_A = qc.compute_reduced_shannon_entropy(processed_data, configuration, 1)\n",
    "print(\"Reduced Shannon entropy: \", reduced_shannon_entropy)\n",
    "\n",
    "# compute reduced shannon entropy on region B of processed data\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "reduced_shannon_entropy_B = qc.compute_reduced_shannon_entropy(processed_data, configuration, 0)\n",
    "print(\"Reduced Shannon entropy: \", reduced_shannon_entropy)\n",
    "\n",
    "# Get MI directly from the shannon entropies\n",
    "\n",
    "MI = reduced_shannon_entropy_A + reduced_shannon_entropy_B - shannon_entropy\n",
    "print(\"Mutual information computed with steps: \", MI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropy \n",
    "\n",
    "Another measure you can calculate directly is the conditional entropy over a region which is defined as:\n",
    "\n",
    "CE(A) = $S_{x}(AB)$ - $S_{x}(B)$\n",
    "\n",
    "You can calculate this in the code as follows:\n",
    "\n",
    "This code assumes region A is defined by 1's and region B is defined by 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional entropy:  0.6737110180226725\n"
     ]
    }
   ],
   "source": [
    "# compute the conditional entropy of region A given region B\n",
    "configuration = [1,1,1,0,0,0,0,0]\n",
    "conditional_entropy = qc.compute_conditional_entropy(processed_data, configuration)\n",
    "print(\"Conditional entropy: \", conditional_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
