{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Document\n",
    "This document details the use of the QCOM package. The goal is a python package that allows users to interact with Aquila and DMRG data seemlessly and prevent repetitive function writing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from DMRG files\n",
    "If you have data in text files with the form\n",
    "\n",
    "state : count\n",
    "\n",
    "state : count\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Then the function below allows you to easily obtain and return the data as a dictionary and additionally the total count of the values (equals 1 if the values are probabilities) using the file path as input.\n",
    "\n",
    "This function also has a built in progress manager so you can estimate how long the data takes to load in. This is helpful for larger datasets. To track progress use the \"show_progress\" flag and set it equal to True. This flag is always the last parameter passed into the function call. It is set to False by default. \n",
    "\n",
    "Note: \n",
    "\n",
    "You must set the flag in the function call itself like shown below if the function has multiple arguments before show_progress. You can try for yourself but if you just do show_progress = True on a separate line and pass that into the function it will not show progress. This is because parse_file also takes in an additional two arguments, one of which (sample_size) can be represented as a boolean. Thus if you just pass file_path and show_progress with show_progress defined on a previous line, it will read show_progress as the sample size input and it will not show progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip install\n",
    "\n",
    "Make sure you pip install qcom before trying the tutorial. If you haven't already running the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "pip install qcom\n",
    "```\n",
    "\n",
    "Confirm that qcom has been succesfully installed and restart your jupyter before trying to import qcom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the qcom module\n",
    "\n",
    "import qcom as qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Parsing file...\n",
      "Task: Parsing file | Progress: 100.00% | Elapsed: 0.00s | Remaining: 0.00s      \n",
      "Completed: Parsing file. Elapsed time: 0.00 seconds.\n",
      "Total count:  1000000000.0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../example_data/1_billion_3.0_4_rungs.txt\"\n",
    "\n",
    "# Process without sampling\n",
    "processed_data, total_count = qc.parse_file(file_path, show_progress = True)\n",
    "\n",
    "print(\"Total count: \", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files\n",
    "\n",
    "The previous code demonstrated how to parse text files, the following code will tell you how to parse parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Parsing Parquet file...\n",
      "\n",
      "Completed: Parsing Parquet file. Elapsed time: 0.20 seconds.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../example_data/1_billion_3.0_4_rungs.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../example_data/1_billion_3.0_4_rungs.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Process without sampling\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m processed_data, total_count \u001b[38;5;241m=\u001b[39m qc\u001b[38;5;241m.\u001b[39mparse_parq(file_path, show_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal count: \u001b[39m\u001b[38;5;124m\"\u001b[39m, total_count)\n",
      "File \u001b[0;32m~/Desktop/Research/QCOM/qcom/io.py:95\u001b[0m, in \u001b[0;36mparse_parq\u001b[0;34m(file_name, show_progress)\u001b[0m\n\u001b[1;32m     88\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m     ProgressManager\u001b[38;5;241m.\u001b[39mprogress(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing Parquet file\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_steps\u001b[38;5;241m=\u001b[39mtotal_steps)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_progress\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ProgressManager\u001b[38;5;241m.\u001b[39mdummy_context()\n\u001b[1;32m     93\u001b[0m ):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Step 1: Read the Parquet file into a DataFrame.\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(file_name, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[1;32m     97\u001b[0m         ProgressManager\u001b[38;5;241m.\u001b[39mupdate_progress(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.11/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    671\u001b[0m     path,\n\u001b[1;32m    672\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    673\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    674\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    675\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    676\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    677\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    679\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.11/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    266\u001b[0m     path,\n\u001b[1;32m    267\u001b[0m     filesystem,\n\u001b[1;32m    268\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    269\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.11/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    140\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../example_data/1_billion_3.0_4_rungs.parquet'"
     ]
    }
   ],
   "source": [
    "file_path = \"../example_data/1_billion_counts_3.0_4_rungs.parquet\"\n",
    "\n",
    "# Process without sampling\n",
    "\n",
    "processed_data, total_count = qc.parse_parq(file_path, show_progress = True)\n",
    "\n",
    "print(\"Total count: \", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most probable states\n",
    "This is great but it's hard to confirm for very large datasets. To confirm the result we might just be interested in the 10 most probable states. The function below allows us to print out the n most probable states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 01000010, Probability: 389245564.00000000\n",
      " 2.  Bit string: 10000001, Probability: 389223579.00000000\n",
      " 3.  Bit string: 01000001, Probability: 48778624.00000000\n",
      " 4.  Bit string: 10000010, Probability: 48777628.00000000\n",
      " 5.  Bit string: 01000000, Probability: 20562997.00000000\n",
      " 6.  Bit string: 00000001, Probability: 20561460.00000000\n",
      " 7.  Bit string: 00000010, Probability: 20550353.00000000\n",
      " 8.  Bit string: 10000000, Probability: 20549831.00000000\n",
      " 9.  Bit string: 10000100, Probability: 4983517.00000000\n",
      "10.  Bit string: 00100001, Probability: 4981922.00000000\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "qc.print_most_probable_data(processed_data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "If I want to take a random sample of the larger data into a smaller amount I can do so using the sample_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Sampling data...\n",
      "Task: Sampling data | Progress: 100.00% | Elapsed: 0.01s | Remaining: 0.00s     \n",
      "Completed: Sampling data. Elapsed time: 0.01 seconds.\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 01000010, Probability: 0.40700000\n",
      " 2.  Bit string: 10000001, Probability: 0.36700000\n",
      " 3.  Bit string: 10000010, Probability: 0.04800000\n",
      " 4.  Bit string: 01000001, Probability: 0.04600000\n",
      " 5.  Bit string: 10000000, Probability: 0.02800000\n",
      " 6.  Bit string: 01000000, Probability: 0.02200000\n",
      " 7.  Bit string: 00000001, Probability: 0.01900000\n",
      " 8.  Bit string: 00000010, Probability: 0.01700000\n",
      " 9.  Bit string: 00100001, Probability: 0.00700000\n",
      "10.  Bit string: 10000011, Probability: 0.00600000\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "sampled_data = qc.sample_data(processed_data, total_count, sample_size, show_progress= True)\n",
    "qc.print_most_probable_data(sampled_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample as you parse\n",
    "It's actually more efficient to sample as we parse the set. So if you know before hand you only care about 100 randomly sample states it makes no sense to parse the whole thing and then sample it down to 100. Instead as you are parsing you will only grab 100 states. We can do this using the parse_file function by adding the sample_size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"1_billion_3.0_4_rungs.txt\"\n",
    "\n",
    "# Process with sampling\n",
    "\n",
    "sample_size = 1000\n",
    "processed_data, total_count = qc.parse_file(file_path, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "The Aquila device has a readout error rate of 0.08 for the excited state and 0.01 for the ground state. We can simulate this error on our data using the following function\n",
    "\n",
    "NOTE: The current function assumes the default values of 0.08 and 0.01. These are taken as parameters so if future error rates change then we can accurately model those as well. Thus, you do not technically need to pass ground_rate and excited_rate into the function. Although it is good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing errors to the data...\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 10000000, Probability: 0.09090909\n",
      " 2.  Bit string: 01000010, Probability: 0.04545455\n",
      " 3.  Bit string: 00000010, Probability: 0.04545455\n",
      " 4.  Bit string: 10000010, Probability: 0.04545455\n",
      " 5.  Bit string: 00010010, Probability: 0.04545455\n",
      " 6.  Bit string: 01001001, Probability: 0.04545455\n",
      " 7.  Bit string: 01000001, Probability: 0.04545455\n",
      " 8.  Bit string: 10000100, Probability: 0.04545455\n",
      " 9.  Bit string: 01000000, Probability: 0.04545455\n",
      "10.  Bit string: 10000011, Probability: 0.04545455\n"
     ]
    }
   ],
   "source": [
    "ground_rate = 0.01\n",
    "excited_rate = 0.08\n",
    "\n",
    "error_data = qc.introduce_error_data(sampled_data, total_count, ground_rate, excited_rate)\n",
    "qc.print_most_probable_data(error_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data\n",
    "Say you have two data sets and you want to combine them. We can do this using the following funciton, however there are some rules. You can either combine two datasets of probabilities or two datasets of counts. You cannot combine a dataset of probabilities and a dataset of counts as this would make normalizing impossible. Additionally, if you combine two probabilities, the function will automatically normalize. If you combine two datasets of counts, the function will NOT normalize. This is so if users want to combine say 100 sets of count data, they can do so without a problem. They simply need to normalize afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10000010': 0.04772727272727273, '10001000': 0.02322727272727273, '00000100': 0.02372727272727273, '01000010': 0.23322727272727273, '10000110': 0.02372727272727273, '10000001': 0.1935, '11000010': 0.02322727272727273, '01000011': 0.02372727272727273, '11000001': 0.025727272727272727, '01000000': 0.03172727272727273, '00010010': 0.02422727272727273, '00000001': 0.007, '10001001': 0.02372727272727273, '01001000': 0.026227272727272728, '00000010': 0.03172727272727273, '00100011': 0.022727272727272728, '01001001': 0.022727272727272728, '10000000': 0.050954545454545454, '01000001': 0.043727272727272726, '10000100': 0.02422727272727273, '00000000': 0.02372727272727273, '10000011': 0.025227272727272727, '00100001': 0.001, '00000011': 0.02322727272727273}\n"
     ]
    }
   ],
   "source": [
    "combined_data = qc.combine_datasets(sampled_data, error_data)\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to txt File\n",
    "If we have a set of data we would like to save to a file we can do so using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"error_data.txt\"\n",
    "qc.save_data(processed_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
