{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Document\n",
    "This document details the use of the Multi Proxy library. The goal is a python package that allows users to interact with Aquila and DMRG data seemlessly and prevent repetitive function writing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data from DMRG files\n",
    "If you have data in text files with the form\n",
    "\n",
    "state : count\n",
    "\n",
    "state : count\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Then the function below allows you to easily obtain and return the data as a dictionary and additionally the total count of the values (equals 1 if the values are probabilities) using the file path as input.\n",
    "\n",
    "This function also has a built in progress manager so you can estimate how long the data takes to load in. This is helpful for larger datasets. To track progress use the \"show_progress\" flag and set it equal to True. This flag is always the last parameter passed into the function call. It is set to False by default. \n",
    "\n",
    "Note: \n",
    "\n",
    "You must set the flag in the function call itself like shown below if the function has multiple arguments before show_progress. You can try for yourself but if you just do show_progress = True on a separate line and pass that into the function it will not show progress. This is because parse_file also takes in an additional two arguments, one of which (sample_size) can be represented as a boolean. Thus if you just pass file_path and show_progress with show_progress defined on a previous line, it will read show_progress as the sample size input and it will not show progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Processor import parse_file\n",
    "\n",
    "file_path = \"../EntanglementCalculation/1_billion_shots/Rba_2.0/16_rungs.txt\"\n",
    "\n",
    "# Process without sampling\n",
    "processed_data, total_count = parse_file(file_path, show_progress = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most probable states\n",
    "This is great but it's hard to confirm for very large datasets. To confirm the result we might just be interested in the 10 most probable states. The function below allows us to print out the n most probable states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 11000110011001100110011001100011, Probability: 0.00534007\n",
      " 2.  Bit string: 11001001100110011001100110010011, Probability: 0.00533582\n",
      " 3.  Bit string: 11001100100110011001100110010011, Probability: 0.00224794\n",
      " 4.  Bit string: 11000110011001100110011000110011, Probability: 0.00224652\n",
      " 5.  Bit string: 11001001100110011001100100110011, Probability: 0.00224516\n",
      " 6.  Bit string: 11001100011001100110011001100011, Probability: 0.00224463\n",
      " 7.  Bit string: 11001001100110011000100110010011, Probability: 0.00179124\n",
      " 8.  Bit string: 11001001100100011001100110010011, Probability: 0.00178862\n",
      " 9.  Bit string: 11000110011001100100011001100011, Probability: 0.00178772\n",
      "10.  Bit string: 11000110011001100010011001100011, Probability: 0.00178722\n"
     ]
    }
   ],
   "source": [
    "from Processor import print_most_probable_data\n",
    "\n",
    "n = 10\n",
    "print_most_probable_data(processed_data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "If I want to take a random sample of the larger data into a smaller amount I can do so using the sample_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Sampling data...\n",
      "Task: Sampling data | Progress: 100.00% | Elapsed: 0.19s | Remaining: 0.00s     \n",
      "Completed: Sampling data. Elapsed time: 0.19 seconds.\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 11001001100110011001100110010011, Probability: 0.00600000\n",
      " 2.  Bit string: 11001100011001100110011001000011, Probability: 0.00400000\n",
      " 3.  Bit string: 11000110001001100110011001100011, Probability: 0.00400000\n",
      " 4.  Bit string: 11001001100110010001100110010011, Probability: 0.00400000\n",
      " 5.  Bit string: 11000110001001100110010010010011, Probability: 0.00300000\n",
      " 6.  Bit string: 11000110011001100100011001100011, Probability: 0.00300000\n",
      " 7.  Bit string: 11000110011001001100011001100011, Probability: 0.00300000\n",
      " 8.  Bit string: 11001100100110011001100110010011, Probability: 0.00300000\n",
      " 9.  Bit string: 11001001100110011001000110010011, Probability: 0.00300000\n",
      "10.  Bit string: 11001001100110010010000110010011, Probability: 0.00300000\n"
     ]
    }
   ],
   "source": [
    "from Processor import sample_data\n",
    "from Processor import print_most_probable_data\n",
    "\n",
    "sample_size = 1000\n",
    "sampled_data = sample_data(processed_data, total_count, sample_size, show_progress= True)\n",
    "print_most_probable_data(sampled_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample as you parse\n",
    "It's actually more efficient to sample as we parse the set. So if you know before hand you only care about 100 randomly sample states it makes no sense to parse the whole thing and then sample it down to 100. Instead as you are parsing you will only grab 100 states. We can do this using the parse_file function by adding the sample_size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Processor import parse_file\n",
    "\n",
    "file_path = \"../EntanglementCalculation/1_billion_shots/Rba_2.0/14_rungs.txt\"\n",
    "\n",
    "# Process with sampling\n",
    "\n",
    "sample_size = 1000\n",
    "processed_data, total_count = parse_file(file_path, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n",
    "The Aquila device has a readout error rate of 0.08 for the excited state and 0.01 for the ground state. We can simulate this error on our data using the following function\n",
    "\n",
    "NOTE: The current function assumes the default values of 0.08 and 0.01. These are taken as parameters so if future error rates change then we can accurately model those as well. Thus, you do not technically need to pass ground_rate and excited_rate into the function. Although it is good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing errors to the data...\n",
      "Most probable 10 bit strings:\n",
      " 1.  Bit string: 0000000110010011000110010011, Probability: 0.00909091\n",
      " 2.  Bit string: 1100110011000001001100010011, Probability: 0.00909091\n",
      " 3.  Bit string: 1000110000010000100100100110, Probability: 0.00909091\n",
      " 4.  Bit string: 1100100000101001100010000011, Probability: 0.00909091\n",
      " 5.  Bit string: 0100000110011000100100010010, Probability: 0.00909091\n",
      " 6.  Bit string: 0100100100110000100110010010, Probability: 0.00909091\n",
      " 7.  Bit string: 0100100000010001000110010001, Probability: 0.00909091\n",
      " 8.  Bit string: 1100110000000000000001000010, Probability: 0.00909091\n",
      " 9.  Bit string: 1100010000110010010010010011, Probability: 0.00909091\n",
      "10.  Bit string: 1000011001000100010010000010, Probability: 0.00909091\n"
     ]
    }
   ],
   "source": [
    "from Processor import introduce_error_data\n",
    "from Processor import print_most_probable_data\n",
    "\n",
    "ground_rate = 0.01\n",
    "excited_rate = 0.08\n",
    "\n",
    "error_data = introduce_error_data(sampled_data, total_count, ground_rate, excited_rate)\n",
    "print_most_probable_data(error_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data\n",
    "Say you have two data sets and you want to combine them. We can do this using the following funciton, however there are some rules. You can either combine two datasets of probabilities or two datasets of counts. You cannot combine a dataset of probabilities and a dataset of counts as this would make normalizing impossible. Additionally, if you combine two probabilities, the function will automatically normalize. If you combine two datasets of counts, the function will NOT normalize. This is so if users want to combine say 100 sets of count data, they can do so without a problem. They simply need to normalize afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1101010011000000100110000011': 0.004545454545454558, '1100110010011000100001001001': 0.0005000000000000014, '1100110011000001001100010011': 0.03654545454545465, '1100100011000100011001000010': 0.004545454545454558, '1100001001001001000010010001': 0.0005000000000000014, '0100100010010000000110010010': 0.012500000000000035, '1100011001100110000000100000': 0.012045454545454578, '1100010000100010010010010011': 0.03300000000000009, '0100000010010000110010010010': 0.0005000000000000014, '0100010000000011001100000011': 0.004545454545454558, '0100000110010011000010010011': 0.013500000000000038, '0100110010001100011000010010': 0.004545454545454558, '1100001101100100011001100010': 0.0005000000000000014, '0100100100110000100110010010': 0.03904545454545466, '1100110000011001100110011000': 0.0050454545454545596, '1000100010001100110001000010': 0.0005000000000000014, '1100011000010001000100000010': 0.004545454545454558, '1100010000010010000110010010': 0.0050454545454545596, '1100011001000000100011001001': 0.0050454545454545596, '1000100011000011001000100011': 0.0005000000000000014, '1001000011001001001000010011': 0.0005000000000000014, '1100011001100100100000100011': 0.0005000000000000014, '1100110000001100001100110000': 0.0050454545454545596, '0100010000001000000010010011': 0.008045454545454567, '1100010011001100100100010001': 0.004545454545454558, '1100100001000001111000110011': 0.004545454545454558, '0000000010000000010010010010': 0.004545454545454558, '1100100011000011000000110001': 0.0050454545454545596, '0000000110010011000110010011': 0.004545454545454558, '1100110011000100001100100011': 0.0005000000000000014, '1100000000110000001000010001': 0.004545454545454558, '1100100010010010000000000011': 0.0065454545454545635, '1100010010000000001000110000': 0.004545454545454558, '1000001001001001000010010001': 0.004545454545454558, '1100100011001001100000000011': 0.0005000000000000014, '1100100001000001100110000010': 0.0050454545454545596, '0010100001000000010010010011': 0.004545454545454558, '1000100011000010011000000011': 0.004545454545454558, '1000010000110011001100010011': 0.008045454545454567, '1100000000110001001100010001': 0.0010000000000000028, '0100001000011000100011000011': 0.005500000000000015, '0110000000110000000100010011': 0.004545454545454558, '0100100000010001000110010001': 0.009045454545454568, '1000100011000001001000100011': 0.004545454545454558, '0100010010000110000000110011': 0.0005000000000000014, '1000000010001000011000100011': 0.002500000000000007, '1000100100010001000110010000': 0.004545454545454558, '1000100110001100010001000010': 0.0010000000000000028, '1101110011000001100110000011': 0.0015000000000000042, '1100100100100001100100100100': 0.0050454545454545596, '1100010000110010010010010011': 0.004545454545454558, '0110001001100100011000000001': 0.004545454545454558, '0000110010001001100000100001': 0.004545454545454558, '0100000010011001000011001001': 0.0005000000000000014, '1000011001000100010010000010': 0.004545454545454558, '1000110001001000011000110011': 0.0005000000000000014, '1100100100101001100010000011': 0.018000000000000047, '1001110000011001100110010011': 0.0010000000000000028, '1100001100100110001000000001': 0.0005000000000000014, '1000000011000010011001000011': 0.007045454545454566, '1100100110000100100101000110': 0.004545454545454558, '1100011001100010010000000110': 0.0050454545454545596, '1100110010000100001001000010': 0.0015000000000000042, '1100110010001000100000001001': 0.004545454545454558, '1100100110011000100000110000': 0.0005000000000000014, '0000010010011000110000010001': 0.004545454545454558, '1100100011000000011001100010': 0.0030000000000000083, '0100011000100011000100110001': 0.004545454545454558, '0100001000111000000011000011': 0.004545454545454558, '1000110000010000100100100110': 0.053545454545454695, '0100001100000110000001000011': 0.0050454545454545596, '1000000010001000111000100001': 0.004545454545454558, '0001110000011001100110010011': 0.004545454545454558, '0100011000000100100000110011': 0.004545454545454558, '1000011000010000100100110000': 0.005545454545454561, '0110011001000001100100010010': 0.07950000000000022, '1100000011001001100000000011': 0.004545454545454558, '1100010010000100001000110010': 0.0010000000000000028, '1100001000001001000000110011': 0.0005000000000000014, '0000011001001100000010010001': 0.004545454545454558, '1100110010001100011000010010': 0.0010000000000000028, '1100010010000110001001100000': 0.0005000000000000014, '1000100100110001000110010000': 0.0005000000000000014, '1100011000000000001100110001': 0.004545454545454558, '1001000011001001001000110010': 0.004545454545454558, '0000000110010010001100100011': 0.004545454545454558, '0110000100110000000100010011': 0.006000000000000017, '1100100000100000011000011001': 0.004545454545454558, '0110001001100100011001000001': 0.0005000000000000014, '1100100000100100100011000110': 0.0030000000000000083, '0110000011000110011000010010': 0.0065454545454545635, '0100110000110010010000000011': 0.0050454545454545596, '1100000011000110001100110000': 0.0050454545454545596, '0100110010010011001001000001': 0.0005000000000000014, '1100001001100100000001100001': 0.0005000000000000014, '0100110000010011001001000001': 0.004545454545454558, '1100110010010001000100110010': 0.0050454545454545596, '1100100110000100100001000110': 0.0015000000000000042, '1000011001100100011000000001': 0.0005000000000000014, '1100100000000001001000100011': 0.0015000000000000042, '1100100110010010000010010000': 0.0005000000000000014, '1100000000100100100011000110': 0.004545454545454558, '1100001100000100001000001001': 0.004545454545454558, '0000011001100100100000100010': 0.004545454545454558, '0100001101100100001001100010': 0.004545454545454558, '1000011001000100110010000010': 0.015500000000000043, '0100011000110010000100110000': 0.0030000000000000083, '1000100010011001001001100001': 0.0050454545454545596, '1100110000110010011000010001': 0.0005000000000000014, '1001000100010000010010010000': 0.004545454545454558, '1001000100010000010010010010': 0.0015000000000000042, '0100100010010010000001100011': 0.0015000000000000042, '1000110000110000001100100001': 0.0010000000000000028, '1100001100110011001000110001': 0.00854545454545457, '1000001000000110011001000110': 0.0050454545454545596, '1100011000100000001100110001': 0.0010000000000000028, '0100011000100001000110010001': 0.005545454545454561, '1000100001001000011000110011': 0.004545454545454558, '1100100000101001100010000011': 0.004545454545454558, '0110011000000100100000110011': 0.0010000000000000028, '1000110001001100001000000011': 0.007545454545454566, '1100110001100001000100010001': 0.0050454545454545596, '1100001001100100000001100000': 0.004545454545454558, '0000110010001100001000010011': 0.0005000000000000014, '1000110011001000000001000011': 0.0005000000000000014, '1100110000000000000001000010': 0.004545454545454558, '1100010010001100100100010001': 0.0010000000000000028, '0100100010000000000110010000': 0.004545454545454558, '0100000010011001000011001000': 0.004545454545454558, '1100100100000001000110000011': 0.007545454545454566, '0100000110011000100100010010': 0.07154545454545475, '0000010010001100001000010011': 0.004545454545454558, '0000110010011000110010010001': 0.002500000000000007, '1000100011000010011000100011': 0.0005000000000000014, '0000100110010010001100100011': 0.0005000000000000014, '0100011001001000010010010011': 0.0050454545454545596, '1100001001000010011000010010': 0.0050454545454545596, '0000011001001100100110010011': 0.0005000000000000014, '0110000000010011000100110011': 0.007545454545454566, '1100110010000110011001001000': 0.011045454545454577, '0000110010011001100010010000': 0.0005000000000000014, '1100100000001000001100110010': 0.0005000000000000014, '0100100010010010000000100011': 0.004545454545454558, '1100000000001000001100000010': 0.004545454545454558, '0100000000001001000000110011': 0.004545454545454558, '1000100100011000100000110000': 0.004545454545454558, '0100001001100100001100000001': 0.0050454545454545596, '0100010000100110011000100011': 0.0050454545454545596, '1100000001000110011001100000': 0.0050454545454545596, '0000110000110000001100100001': 0.004545454545454558, '1100100000001001001000100011': 0.004545454545454558, '0110011001000000100100110010': 0.004545454545454558, '1100000001100001000110010010': 0.0005000000000000014, '0000110010011001000010010000': 0.004545454545454558, '1010100000001100110001000010': 0.004545454545454558, '0100011001000001100000110001': 0.0050454545454545596, '1100000100011000100001100000': 0.0050454545454545596, '1000000011000110011000100000': 0.007545454545454566, '1000011000000010011001100010': 0.0050454545454545596, '0100010000000011001100010011': 0.0005000000000000014, '0100010010000110001000100000': 0.004545454545454558, '1100011000010001000110000010': 0.0005000000000000014, '1001001100110000100001100011': 0.005500000000000015, '1100100000100010011000011001': 0.0010000000000000028, '0100110011000100000100000011': 0.004545454545454558, '1000011001000000110000100011': 0.006045454545454561, '0110000001100010010010010011': 0.0005000000000000014, '1100000110010010010000010000': 0.004545454545454558, '1011001100000000101001100011': 0.004545454545454558, '1100110000110010010000010001': 0.004545454545454558, '1000100011001000000001000011': 0.004545454545454558, '1000100110001100010001000000': 0.004545454545454558, '1100000001100000000110010010': 0.004545454545454558, '1100100001001001011000110011': 0.01450000000000004, '0100010010000110000000110001': 0.004545454545454558, '1000011001100100010000000001': 0.004545454545454558, '0000110010001001100100100001': 0.0005000000000000014}\n"
     ]
    }
   ],
   "source": [
    "from Processor import combine_datasets\n",
    "\n",
    "combined_data = combine_datasets(sampled_data, error_data)\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to txt File\n",
    "If we have a set of data we would like to save to a file we can do so using the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Saving data...\n",
      "Task: Saving data | Progress: 100.00% | Elapsed: 0.91s | Remaining: 0.00s\n",
      "Completed: Saving data. Elapsed time: 0.91 seconds.\n"
     ]
    }
   ],
   "source": [
    "from Processor import save_data\n",
    "\n",
    "file_path = \"error_data.txt\"\n",
    "save_data(processed_data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
